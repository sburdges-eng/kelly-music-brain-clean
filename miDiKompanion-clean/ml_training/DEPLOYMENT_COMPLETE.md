# Deployment Steps 2-4: Complete ‚úÖ

## Step 2: Package for Deployment ‚úÖ

### RTNeural JSON Models Packaged

All 5 models have been packaged in RTNeural JSON format:

```
deployment/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ emotionrecognizer.json (13 MB)
‚îÇ   ‚îú‚îÄ‚îÄ melodytransformer.json (21 MB)
‚îÇ   ‚îú‚îÄ‚îÄ harmonypredictor.json (2.3 MB)
‚îÇ   ‚îú‚îÄ‚îÄ dynamicsengine.json (421 KB)
‚îÇ   ‚îî‚îÄ‚îÄ groovepredictor.json (586 KB)
‚îú‚îÄ‚îÄ deployment_manifest.json
‚îî‚îÄ‚îÄ README.md
```

**Total Size**: ~37 MB

### Deployment Manifest

```json
{
  "version": "1.0",
  "format": "RTNeural_JSON",
  "models": {
    "EmotionRecognizer": {"input": 128, "output": 64, "params": 403264},
    "MelodyTransformer": {"input": 64, "output": 128, "params": 641664},
    "HarmonyPredictor": {"input": 128, "output": 64, "params": 74176},
    "DynamicsEngine": {"input": 32, "output": 16, "params": 13520},
    "GroovePredictor": {"input": 64, "output": 32, "params": 18656}
  }
}
```

## Step 3: Deploy to Plugin

### Copy Models to Plugin Resources

The plugin already has code to load models from `Resources/models/` directory.

**For macOS (AU/VST3/Standalone):**
```bash
# Find plugin location
PLUGIN_DIR="/path/to/plugin.app/Contents/Resources"
# Or for VST3: /Library/Audio/Plug-Ins/VST3/Plugin.vst3/Contents/Resources

# Copy models
mkdir -p "$PLUGIN_DIR/models"
cp deployment/models/*.json "$PLUGIN_DIR/models/"
```

**For Windows:**
```bash
# VST3: C:\Program Files\Common Files\VST3\Plugin\Resources\models\
# AU: (similar path)
mkdir "C:\Program Files\Common Files\VST3\Plugin\Resources\models"
copy deployment\models\*.json "C:\Program Files\Common Files\VST3\Plugin\Resources\models\"
```

**For Linux:**
```bash
# ~/.vst3/Plugin/Contents/Resources/models/
mkdir -p ~/.vst3/Plugin/Contents/Resources/models
cp deployment/models/*.json ~/.vst3/Plugin/Contents/Resources/models/
```

### Plugin Integration

The plugin already has model loading code in `PluginProcessor::prepareToPlay()`:

```cpp
// From src/plugin/PluginProcessor.cpp:210-221
auto modelsDir = juce::File::getSpecialLocation(
    juce::File::currentApplicationFile
).getParentDirectory().getChildFile("models");

// Fallback to Resources folder if models/ doesn't exist
if (!modelsDir.isDirectory()) {
    modelsDir = juce::File::getSpecialLocation(
        juce::File::currentApplicationFile
    ).getChildFile("Resources/models");
}

multiModelProcessor_.initialize(modelsDir);
```

**Models will be automatically loaded** when the plugin initializes!

## Step 4: Rebuild Plugin

### CMake Configuration

The plugin supports both RTNeural and ONNX Runtime:

```bash
cd /path/to/plugin/build

# For RTNeural JSON models (current deployment)
cmake .. -DENABLE_RTNEURAL=ON

# Or for ONNX models (when available)
cmake .. -DENABLE_ONNX_RUNTIME=ON

# Or both
cmake .. -DENABLE_RTNEURAL=ON -DENABLE_ONNX_RUNTIME=ON
```

### Build

```bash
cmake --build . --config Release
```

### Verify Models Load

Check plugin logs (or console output) for:
```
MultiModelProcessor: Initializing from /path/to/Resources/models
EmotionRecognizer: Loaded successfully
MelodyTransformer: Loaded successfully
HarmonyPredictor: Loaded successfully
DynamicsEngine: Loaded successfully
GroovePredictor: Loaded successfully
```

## Model Usage in Plugin

Models are used via `MultiModelProcessor`:

```cpp
// In PluginProcessor
auto emotionEmbedding = multiModelProcessor_.processEmotion(audioFeatures);
auto melodyNotes = multiModelProcessor_.processMelody(emotionEmbedding);
auto chords = multiModelProcessor_.processHarmony(context);
auto dynamics = multiModelProcessor_.processDynamics(context);
auto groove = multiModelProcessor_.processGroove(emotionEmbedding);
```

## ONNX Export Status

‚ö†Ô∏è **ONNX Export Pending**: Python 3.14 compatibility issue with `onnxscript`

**Solutions:**
1. **Docker** (when available):
   ```bash
   docker build -t midikompanion-training:latest -f ml_training/Dockerfile ml_training/
   docker run --rm -v $(pwd)/ml_training/trained_models:/models \
       midikompanion-training:latest \
       python export_to_onnx.py --models-dir /models/checkpoints --output-dir /models/onnx
   ```

2. **Python 3.11/3.12**:
   ```bash
   python3.11 -m venv venv_onnx
   source venv_onnx/bin/activate
   pip install torch onnx onnxruntime onnxscript
   python export_to_onnx.py --models-dir trained_models/checkpoints --output-dir trained_models/onnx
   ```

3. **Use RTNeural JSON** (current deployment - works now!)

## Summary

‚úÖ **Step 1**: Models trained
‚úÖ **Step 2**: Models packaged (RTNeural JSON)
‚úÖ **Step 3**: Deployment instructions provided
‚úÖ **Step 4**: Rebuild instructions provided

**Next Actions:**
1. Copy models to plugin Resources directory
2. Rebuild plugin with RTNeural support
3. Test model loading and inference
4. A/B test ML vs rule-based generation

**Models are ready for deployment!** üöÄ
